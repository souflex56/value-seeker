#!/usr/bin/env python3
"""
æµ‹è¯•å¢å¼ºçš„PDFå¤„ç†åŠŸèƒ½
å¯¹æ¯”åŸæœ‰ç³»ç»Ÿå’Œä¸­æ–‡ä¼˜åŒ–ç³»ç»Ÿçš„æ•ˆæœ
"""

import sys
import os
from pathlib import Path
sys.path.append('.')

def test_enhanced_pdf_processing():
    """æµ‹è¯•å¢å¼ºçš„PDFå¤„ç†åŠŸèƒ½"""
    try:
        from src.data.enhanced_document_processor import EnhancedDocumentProcessor
        
        print("ğŸ‡¨ğŸ‡³ æµ‹è¯•å¢å¼ºçš„PDFå¤„ç†ç³»ç»Ÿï¼ˆä¸­æ–‡ä¼˜åŒ–ï¼‰")
        print("="*60)
        
        # é…ç½®
        config = {
            'chunk_size': 1000,
            'chunk_overlap': 200,
            'min_chunk_size': 100,
            'pdf_strategy': 'fast',
            'use_high_res': False,
            'enable_chinese_optimization': True,
            'chinese_detection_threshold': 0.3,
            'preserve_sentences': True,
            'preserve_financial_terms': True,
            'use_semantic_split': True
        }
        
        # åˆå§‹åŒ–å¢å¼ºå¤„ç†å™¨
        processor = EnhancedDocumentProcessor(config)
        
        # PDFæ–‡ä»¶è·¯å¾„
        pdf_path = "data/reports/600580_å§é¾™ç”µé©±2025-04-26_å§é¾™ç”µé©±2024å¹´å¹´åº¦æŠ¥å‘Š_2105.pdf"
        
        if not os.path.exists(pdf_path):
            print(f"âŒ PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
            return False
        
        print(f"ğŸ” å¼€å§‹å¤„ç†PDF: {pdf_path}")
        print(f"ğŸ“„ æ–‡ä»¶å¤§å°: {os.path.getsize(pdf_path) / 1024 / 1024:.2f} MB")
        
        # è§£æPDF
        print("\nğŸ“– æ­¥éª¤1: è§£æPDFæ–‡æ¡£...")
        documents = processor.parse_pdf(pdf_path)
        print(f"âœ… è§£æå®Œæˆï¼Œå…±ç”Ÿæˆ {len(documents)} ä¸ªæ–‡æ¡£é¡µé¢")
        
        # æ˜¾ç¤ºå‰å‡ ä¸ªæ–‡æ¡£çš„è¯­è¨€æ£€æµ‹ç»“æœ
        print("\nğŸ” è¯­è¨€æ£€æµ‹ç»“æœ:")
        for i, doc in enumerate(documents[:5]):
            language = processor._detect_language(doc.content)
            print(f"  é¡µé¢ {doc.page_number}: {language} (å†…å®¹é•¿åº¦: {len(doc.content)} å­—ç¬¦)")
        
        # æ–‡æ¡£åˆ†å—ï¼ˆä½¿ç”¨å¢å¼ºåŠŸèƒ½ï¼‰
        print("\nâœ‚ï¸  æ­¥éª¤2: å¢å¼ºæ–‡æ¡£åˆ†å—...")
        chunks = processor.chunk_documents(documents)
        print(f"âœ… åˆ†å—å®Œæˆï¼Œå…±ç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æ¡£å—")
        
        # è·å–å¤„ç†ç»Ÿè®¡
        stats = processor.get_processing_stats(chunks)
        print("\nğŸ“Š å¤„ç†ç»Ÿè®¡:")
        print(f"  æ€»å—æ•°: {stats['total_chunks']}")
        print(f"  å¹³å‡å—é•¿åº¦: {stats['avg_chunk_length']:.0f} å­—ç¬¦")
        print(f"  è´¢åŠ¡ç›¸å…³å—: {stats['financial_chunks']}")
        print(f"  åˆ†å—æ–¹æ³•åˆ†å¸ƒ: {stats['chunking_methods']}")
        print(f"  è¯­è¨€åˆ†å¸ƒ: {stats['languages']}")
        
        # æ˜¾ç¤ºå‰å‡ ä¸ªå—çš„è¯¦ç»†ä¿¡æ¯
        print("\nğŸ“ å—ä¿¡æ¯é¢„è§ˆ:")
        for i, chunk in enumerate(chunks[:5]):
            print(f"  å— {i+1} (ID: {chunk.chunk_id}):")
            print(f"    é¡µç : {chunk.metadata.get('page_number', 'N/A')}")
            print(f"    é•¿åº¦: {len(chunk.content)} å­—ç¬¦")
            print(f"    åˆ†å—æ–¹æ³•: {chunk.metadata.get('chunking_method', 'N/A')}")
            print(f"    è¯­è¨€: {chunk.metadata.get('language', 'N/A')}")
            print(f"    è´¢åŠ¡æœ¯è¯­æ•°: {chunk.metadata.get('financial_terms_count', 0)}")
            print(f"    æ•°å­—æ•°é‡: {chunk.metadata.get('numbers_count', 0)}")
            print(f"    å†…å®¹é¢„è§ˆ: {chunk.content[:80].replace(chr(10), ' ')}...")
            print()
        
        # åˆ†å—æ–¹æ³•å¯¹æ¯”ï¼ˆé€‰æ‹©ä¸€ä¸ªæœ‰ä»£è¡¨æ€§çš„æ–‡æ¡£ï¼‰
        if documents:
            print("ğŸ”„ æ­¥éª¤3: åˆ†å—æ–¹æ³•å¯¹æ¯”...")
            comparison = processor.compare_chunking_methods(documents[10])  # é€‰æ‹©ç¬¬11é¡µ
            
            print("ğŸ“ˆ åˆ†å—æ–¹æ³•å¯¹æ¯”ç»“æœ:")
            for method, result in comparison.items():
                print(f"  {method}:")
                print(f"    å—æ•°é‡: {result['chunk_count']}")
                print(f"    å¹³å‡é•¿åº¦: {result['avg_length']:.0f} å­—ç¬¦")
                if 'financial_chunks' in result:
                    print(f"    è´¢åŠ¡ç›¸å…³å—: {result['financial_chunks']}")
                print()
        
        print("ğŸ‰ å¢å¼ºPDFå¤„ç†æµ‹è¯•å®Œæˆ!")
        return True
        
    except Exception as e:
        print(f"âŒ å¢å¼ºPDFå¤„ç†æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_chinese_text_splitter():
    """å•ç‹¬æµ‹è¯•ä¸­æ–‡æ–‡æœ¬åˆ†å‰²å™¨"""
    try:
        from src.data.chinese_text_splitter import create_chinese_text_splitter
        
        print("\n" + "="*60)
        print("ğŸ”¤ æµ‹è¯•ä¸­æ–‡æ–‡æœ¬åˆ†å‰²å™¨")
        print("="*60)
        
        # æµ‹è¯•æ–‡æœ¬ï¼ˆæ¨¡æ‹Ÿè´¢åŠ¡æŠ¥å‘Šå†…å®¹ï¼‰
        test_text = """
        å§é¾™ç”µæ°”é©±åŠ¨é›†å›¢è‚¡ä»½æœ‰é™å…¬å¸2024å¹´å¹´åº¦æŠ¥å‘Šæ˜¾ç¤ºï¼Œå…¬å¸è¥ä¸šæ”¶å…¥è¾¾åˆ°150.5äº¿å…ƒï¼ŒåŒæ¯”å¢é•¿12.3%ã€‚
        å‡€åˆ©æ¶¦ä¸º8.2äº¿å…ƒï¼Œè¾ƒä¸Šå¹´åŒæœŸå¢é•¿15.7%ã€‚æ¯›åˆ©ç‡ä¿æŒåœ¨23.4%çš„è¾ƒé«˜æ°´å¹³ã€‚
        
        å…¬å¸èµ„äº§è´Ÿå€ºç‡ä¸º45.2%ï¼ŒæµåŠ¨æ¯”ç‡ä¸º1.85ï¼Œé€ŸåŠ¨æ¯”ç‡ä¸º1.42ï¼Œè´¢åŠ¡çŠ¶å†µè‰¯å¥½ã€‚
        æ¯è‚¡æ”¶ç›Šä¸º1.23å…ƒï¼Œæ¯è‚¡å‡€èµ„äº§ä¸º8.95å…ƒï¼Œå‡€èµ„äº§æ”¶ç›Šç‡è¾¾åˆ°13.7%ã€‚
        
        åœ¨ç°é‡‘æµæ–¹é¢ï¼Œç»è¥æ´»åŠ¨ç°é‡‘æµå‡€é¢ä¸º12.8äº¿å…ƒï¼ŒæŠ•èµ„æ´»åŠ¨ç°é‡‘æµå‡€é¢ä¸º-5.6äº¿å…ƒï¼Œ
        ç­¹èµ„æ´»åŠ¨ç°é‡‘æµå‡€é¢ä¸º-3.2äº¿å…ƒï¼Œè‡ªç”±ç°é‡‘æµä¸º7.2äº¿å…ƒã€‚
        
        å±•æœ›æœªæ¥ï¼Œå…¬å¸å°†ç»§ç»­ä¸“æ³¨äºç”µæœºé©±åŠ¨æŠ€æœ¯çš„åˆ›æ–°å‘å±•ï¼Œæå‡äº§å“ç«äº‰åŠ›ï¼Œ
        æ‰©å¤§å¸‚åœºä»½é¢ï¼Œä¸ºè‚¡ä¸œåˆ›é€ æ›´å¤§ä»·å€¼ã€‚
        """
        
        # åˆ›å»ºä¸­æ–‡åˆ†å‰²å™¨
        config = {
            'chunk_size': 200,  # è¾ƒå°çš„å—ç”¨äºæµ‹è¯•
            'chunk_overlap': 50,
            'min_chunk_size': 50,
            'preserve_sentences': True,
            'preserve_financial_terms': True,
            'use_semantic_split': True
        }
        
        splitter = create_chinese_text_splitter(config)
        
        # åˆ†å‰²æ–‡æœ¬
        chunks = splitter.split_text(test_text)
        
        print(f"âœ… åˆ†å‰²å®Œæˆï¼Œå…±ç”Ÿæˆ {len(chunks)} ä¸ªå—")
        
        # æ˜¾ç¤ºåˆ†å‰²ç»“æœ
        print("\nğŸ“ åˆ†å‰²ç»“æœ:")
        for i, chunk in enumerate(chunks):
            metadata = splitter.get_chunk_metadata(chunk, i)
            print(f"  å— {i+1}:")
            print(f"    é•¿åº¦: {len(chunk)} å­—ç¬¦")
            print(f"    è´¢åŠ¡æœ¯è¯­: {metadata['financial_terms_count']}")
            print(f"    æ•°å­—: {metadata['numbers_count']}")
            print(f"    å†…å®¹: {chunk.strip()}")
            print()
        
        print("ğŸ‰ ä¸­æ–‡æ–‡æœ¬åˆ†å‰²å™¨æµ‹è¯•å®Œæˆ!")
        return True
        
    except Exception as e:
        print(f"âŒ ä¸­æ–‡æ–‡æœ¬åˆ†å‰²å™¨æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def compare_with_original():
    """ä¸åŸæœ‰ç³»ç»Ÿå¯¹æ¯”"""
    try:
        from src.data.document_processor import DocumentProcessor
        from src.data.enhanced_document_processor import EnhancedDocumentProcessor
        
        print("\n" + "="*60)
        print("âš–ï¸  åŸæœ‰ç³»ç»Ÿ vs å¢å¼ºç³»ç»Ÿå¯¹æ¯”")
        print("="*60)
        
        pdf_path = "data/reports/600580_å§é¾™ç”µé©±2025-04-26_å§é¾™ç”µé©±2024å¹´å¹´åº¦æŠ¥å‘Š_2105.pdf"
        
        if not os.path.exists(pdf_path):
            print(f"âŒ PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
            return False
        
        # é…ç½®
        config = {
            'chunk_size': 1000,
            'chunk_overlap': 200,
            'min_chunk_size': 100,
            'pdf_strategy': 'fast',
        }
        
        # åŸæœ‰ç³»ç»Ÿ
        print("ğŸ“„ æµ‹è¯•åŸæœ‰ç³»ç»Ÿ...")
        original_processor = DocumentProcessor(config)
        original_docs = original_processor.parse_pdf(pdf_path)
        original_chunks = original_processor.chunk_documents(original_docs[:10])  # åªæµ‹è¯•å‰10é¡µ
        
        # å¢å¼ºç³»ç»Ÿ
        print("ğŸš€ æµ‹è¯•å¢å¼ºç³»ç»Ÿ...")
        enhanced_config = {**config, 'enable_chinese_optimization': True}
        enhanced_processor = EnhancedDocumentProcessor(enhanced_config)
        enhanced_docs = enhanced_processor.parse_pdf(pdf_path)
        enhanced_chunks = enhanced_processor.chunk_documents(enhanced_docs[:10])  # åªæµ‹è¯•å‰10é¡µ
        
        # å¯¹æ¯”ç»“æœ
        print("\nğŸ“Š å¯¹æ¯”ç»“æœ:")
        print(f"åŸæœ‰ç³»ç»Ÿ:")
        print(f"  å—æ•°é‡: {len(original_chunks)}")
        print(f"  å¹³å‡é•¿åº¦: {sum(len(c.content) for c in original_chunks) / len(original_chunks):.0f} å­—ç¬¦")
        print(f"  åˆ†å—æ–¹æ³•: {set(c.metadata.get('chunking_method', 'unknown') for c in original_chunks)}")
        
        print(f"\nå¢å¼ºç³»ç»Ÿ:")
        print(f"  å—æ•°é‡: {len(enhanced_chunks)}")
        print(f"  å¹³å‡é•¿åº¦: {sum(len(c.content) for c in enhanced_chunks) / len(enhanced_chunks):.0f} å­—ç¬¦")
        print(f"  åˆ†å—æ–¹æ³•: {set(c.metadata.get('chunking_method', 'unknown') for c in enhanced_chunks)}")
        
        # è·å–å¢å¼ºç³»ç»Ÿçš„ç»Ÿè®¡
        stats = enhanced_processor.get_processing_stats(enhanced_chunks)
        print(f"  è´¢åŠ¡ç›¸å…³å—: {stats['financial_chunks']}")
        print(f"  è¯­è¨€åˆ†å¸ƒ: {stats['languages']}")
        
        print("\nğŸ‰ ç³»ç»Ÿå¯¹æ¯”å®Œæˆ!")
        return True
        
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿå¯¹æ¯”å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹å¢å¼ºPDFå¤„ç†åŠŸèƒ½æµ‹è¯•")
    print("="*60)
    
    # æµ‹è¯•ä¸­æ–‡æ–‡æœ¬åˆ†å‰²å™¨
    chinese_splitter_success = test_chinese_text_splitter()
    
    # æµ‹è¯•å¢å¼ºç³»ç»Ÿ
    enhanced_success = test_enhanced_pdf_processing()
    
    # ä¸åŸæœ‰ç³»ç»Ÿå¯¹æ¯”
    comparison_success = compare_with_original()
    
    # æ€»ç»“
    print("\n" + "="*60)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“")
    print("="*60)
    print(f"ä¸­æ–‡æ–‡æœ¬åˆ†å‰²å™¨: {'âœ… æˆåŠŸ' if chinese_splitter_success else 'âŒ å¤±è´¥'}")
    print(f"å¢å¼ºPDFå¤„ç†ç³»ç»Ÿ: {'âœ… æˆåŠŸ' if enhanced_success else 'âŒ å¤±è´¥'}")
    print(f"ç³»ç»Ÿå¯¹æ¯”: {'âœ… æˆåŠŸ' if comparison_success else 'âŒ å¤±è´¥'}")
    
    if all([chinese_splitter_success, enhanced_success, comparison_success]):
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å¢å¼ºç³»ç»Ÿå¯ä»¥æŠ•å…¥ä½¿ç”¨ã€‚")
        print("\nğŸ’¡ é›†æˆå»ºè®®:")
        print("1. å°† EnhancedDocumentProcessor æ›¿æ¢ç°æœ‰çš„ DocumentProcessor")
        print("2. åœ¨é…ç½®ä¸­å¯ç”¨ enable_chinese_optimization")
        print("3. æ ¹æ®éœ€è¦è°ƒæ•´ä¸­æ–‡æ£€æµ‹é˜ˆå€¼å’Œåˆ†å—å‚æ•°")
        print("4. ç›‘æ§åˆ†å—æ•ˆæœï¼Œå¿…è¦æ—¶è¿›ä¸€æ­¥ä¼˜åŒ–")
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")

if __name__ == "__main__":
    main()