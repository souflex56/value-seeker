#!/usr/bin/env python3
"""
çˆ¶å­åˆ†å—RAGå¤„ç†å™¨

å®Œæ•´çš„çˆ¶å­åˆ†å—RAGæµç¨‹å®ç°ï¼š
1. ã€è§£æã€‘é«˜ä¿çœŸè¡¨æ ¼æå– - pdfplumberæå–è¡¨æ ¼ï¼Œè½¬ä¸ºMarkdownå­åˆ†å—
2. ã€è§£æã€‘é«˜ä¿çœŸæ–‡æœ¬æå– - unstructuredè·³è¿‡è¡¨æ ¼åŒºåŸŸï¼Œæå–æ–‡æœ¬å…ƒç´ 
3. ã€åˆ†å—ã€‘æ–‡æœ¬çš„äºŒæ¬¡åˆ†å— - é€’å½’å­—ç¬¦åˆ†å—ï¼Œå½¢æˆç»†ç²’åº¦å­åˆ†å—
4. ã€å¯ŒåŒ–ä¸ç´¢å¼•ã€‘æ„å»ºçˆ¶å­å…³ç³» - å®šä¹‰çˆ¶åˆ†å—ï¼Œå…³è”å­åˆ†å—ï¼Œå­˜å‚¨å…ƒæ•°æ®
5. ã€æ£€ç´¢ã€‘æ‰§è¡Œçˆ¶å­æ–‡æ¡£æ£€ç´¢ - æ£€ç´¢å­åˆ†å—ï¼Œè¿”å›çˆ¶åˆ†å—å®Œæ•´å†…å®¹
"""

import sys
import os
import uuid
import json
import re
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from dataclasses import dataclass

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.abspath('.'))

try:
    import pdfplumber
    from unstructured.partition.pdf import partition_pdf
    from unstructured.documents.elements import Table as UnstructuredTable, Text, Title
    import pandas as pd
    DEPENDENCIES_AVAILABLE = True
except ImportError:
    DEPENDENCIES_AVAILABLE = False

@dataclass
class TableChunk:
    """è¡¨æ ¼å­åˆ†å—"""
    chunk_id: str
    content: str  # Markdownæ ¼å¼
    page_number: int
    bbox: Dict[str, float]  # è¾¹ç•Œæ¡†
    table_type: str
    row_count: int
    col_count: int
    parent_id: str

@dataclass
class TextChunk:
    """æ–‡æœ¬å­åˆ†å—"""
    chunk_id: str
    content: str
    page_number: Optional[int]
    element_type: str  # Title, Text, etc.
    parent_id: str
    char_count: int

@dataclass
class ParentChunk:
    """çˆ¶åˆ†å—"""
    parent_id: str
    title: str
    content: str  # å®Œæ•´å†…å®¹
    page_range: Tuple[int, int]
    child_ids: List[str]
    chunk_type: str  # chapter, section, page_group

class ParentChildRAGProcessor:
    """çˆ¶å­åˆ†å—RAGå¤„ç†å™¨"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        
        # è¡¨æ ¼æå–é…ç½®
        self.table_settings = {
            "vertical_strategy": "lines",
            "horizontal_strategy": "lines",
            "snap_tolerance": 3,
            "join_tolerance": 3,
            "edge_min_length": 3
        }
        
        # æ–‡æœ¬åˆ†å—é…ç½®
        self.child_chunk_size = self.config.get('child_chunk_size', 800)
        self.child_chunk_overlap = self.config.get('child_chunk_overlap', 100)
        
        # çˆ¶åˆ†å—é…ç½®
        self.parent_strategy = self.config.get('parent_strategy', 'page_group')  # chapter, section, page_group
        self.pages_per_parent = self.config.get('pages_per_parent', 3)
        
        if not DEPENDENCIES_AVAILABLE:
            raise ImportError("éœ€è¦å®‰è£…: pip install pdfplumber unstructured pandas")
    
    def process_pdf(self, pdf_path: str) -> Dict[str, Any]:
        """
        å®Œæ•´çš„çˆ¶å­åˆ†å—RAGæµç¨‹
        
        Returns:
            {
                'child_chunks': List[Dict],  # æ‰€æœ‰å­åˆ†å—ï¼ˆç”¨äºå‘é‡åŒ–ï¼‰
                'parent_chunks': Dict[str, Dict],  # çˆ¶åˆ†å—å­˜å‚¨ï¼ˆé”®å€¼å¯¹ï¼‰
                'metadata': Dict,
                'stats': Dict
            }
        """
        print("ğŸš€ å¼€å§‹çˆ¶å­åˆ†å—RAGå¤„ç†...")
        
        # Step 1: é«˜ä¿çœŸè¡¨æ ¼æå–
        print("ğŸ“Š Step 1: é«˜ä¿çœŸè¡¨æ ¼æå–...")
        table_chunks, table_bboxes = self._extract_table_chunks(pdf_path)
        print(f"   âœ“ æå–åˆ° {len(table_chunks)} ä¸ªè¡¨æ ¼å­åˆ†å—")
        
        # Step 2: é«˜ä¿çœŸæ–‡æœ¬æå–ï¼ˆè·³è¿‡è¡¨æ ¼åŒºåŸŸï¼‰
        print("ğŸ“ Step 2: é«˜ä¿çœŸæ–‡æœ¬æå–...")
        text_elements = self._extract_text_elements(pdf_path, table_bboxes)
        print(f"   âœ“ æå–åˆ° {len(text_elements)} ä¸ªæ–‡æœ¬å…ƒç´ ")
        
        # Step 3: æ–‡æœ¬çš„äºŒæ¬¡åˆ†å—
        print("âœ‚ï¸  Step 3: æ–‡æœ¬çš„äºŒæ¬¡åˆ†å—...")
        text_chunks = self._chunk_text_elements(text_elements)
        print(f"   âœ“ ç”Ÿæˆ {len(text_chunks)} ä¸ªæ–‡æœ¬å­åˆ†å—")
        
        # Step 4: æ„å»ºçˆ¶å­å…³ç³»
        print("ğŸ”— Step 4: æ„å»ºçˆ¶å­å…³ç³»...")
        parent_chunks = self._build_parent_child_relationships(table_chunks, text_chunks)
        print(f"   âœ“ æ„å»º {len(parent_chunks)} ä¸ªçˆ¶åˆ†å—")
        
        # Step 5: å‡†å¤‡RAGå­˜å‚¨æ ¼å¼
        print("ğŸ’¾ Step 5: å‡†å¤‡RAGå­˜å‚¨æ ¼å¼...")
        child_chunks_for_vector_db = self._prepare_child_chunks_for_vector_db(table_chunks, text_chunks)
        parent_chunks_for_doc_store = self._prepare_parent_chunks_for_doc_store(parent_chunks)
        
        result = {
            'child_chunks': child_chunks_for_vector_db,  # ç”¨äºå‘é‡æ•°æ®åº“
            'parent_chunks': parent_chunks_for_doc_store,  # ç”¨äºæ–‡æ¡£å­˜å‚¨
            'metadata': {
                'source_file': pdf_path,
                'processing_method': 'parent_child_rag',
                'config': self.config,
                'processed_at': datetime.now().isoformat()
            },
            'stats': {
                'total_child_chunks': len(child_chunks_for_vector_db),
                'table_child_chunks': len(table_chunks),
                'text_child_chunks': len(text_chunks),
                'parent_chunks': len(parent_chunks),
                'table_bboxes_excluded': len(table_bboxes)
            }
        }
        
        print("âœ… çˆ¶å­åˆ†å—RAGå¤„ç†å®Œæˆï¼")
        return result
    
    def _extract_table_chunks(self, pdf_path: str) -> Tuple[List[TableChunk], List[Dict]]:
        """Step 1: ä½¿ç”¨pdfplumberæå–è¡¨æ ¼ï¼Œè½¬ä¸ºMarkdownå­åˆ†å—"""
        table_chunks = []
        table_bboxes = []  # ç”¨äºå‘Šè¯‰unstructuredè·³è¿‡è¿™äº›åŒºåŸŸ
        
        with pdfplumber.open(pdf_path) as pdf:
            for page_num, page in enumerate(pdf.pages, 1):
                tables = page.find_tables(table_settings=self.table_settings)
                
                for table_idx, table in enumerate(tables):
                    try:
                        # æå–è¡¨æ ¼æ•°æ®
                        raw_data = table.extract()
                        if not raw_data or len(raw_data) < 2:
                            continue
                        
                        # è½¬æ¢ä¸ºDataFrame
                        df = pd.DataFrame(raw_data[1:], columns=raw_data[0])
                        df = df.dropna(how='all').dropna(axis=1, how='all')
                        
                        if df.empty:
                            continue
                        
                        # è½¬æ¢ä¸ºMarkdown
                        markdown_content = self._dataframe_to_markdown(df)
                        
                        # è®°å½•è¾¹ç•Œæ¡†ï¼ˆç”¨äºunstructuredè·³è¿‡ï¼‰
                        bbox = {
                            'page': page_num,
                            'x0': float(table.bbox[0]),
                            'y0': float(table.bbox[1]),
                            'x1': float(table.bbox[2]),
                            'y1': float(table.bbox[3])
                        }
                        table_bboxes.append(bbox)
                        
                        # åˆ›å»ºè¡¨æ ¼å­åˆ†å—ï¼ˆæš‚æ—¶æ²¡æœ‰parent_idï¼Œåç»­åˆ†é…ï¼‰
                        table_chunk = TableChunk(
                            chunk_id=f"table_{page_num}_{table_idx}_{str(uuid.uuid4())[:8]}",
                            content=markdown_content,
                            page_number=page_num,
                            bbox=bbox,
                            table_type=self._classify_table_type(df),
                            row_count=len(df),
                            col_count=len(df.columns),
                            parent_id=""  # åç»­åˆ†é…
                        )
                        
                        table_chunks.append(table_chunk)
                        
                    except Exception as e:
                        print(f"   âš ï¸  é¡µé¢{page_num}è¡¨æ ¼{table_idx}å¤„ç†å¤±è´¥: {e}")
                        continue
        
        return table_chunks, table_bboxes
    
    def _extract_text_elements(self, pdf_path: str, table_bboxes: List[Dict]) -> List[Dict]:
        """Step 2: ä½¿ç”¨unstructuredæå–æ–‡æœ¬ï¼Œè·³è¿‡è¡¨æ ¼åŒºåŸŸ"""
        try:
            # ä½¿ç”¨unstructuredè§£æPDF
            elements = partition_pdf(
                filename=pdf_path,
                strategy="fast",
                infer_table_structure=False,  # å…³é”®ï¼šä¸è§£æè¡¨æ ¼
                extract_images_in_pdf=False,
                include_page_breaks=True
            )
            
            # è¿‡æ»¤æ‰è¡¨æ ¼åŒºåŸŸçš„æ–‡æœ¬
            text_elements = []
            for element in elements:
                if not hasattr(element, 'text') or not element.text.strip():
                    continue
                
                text = element.text.strip()
                
                # è·³è¿‡å¯èƒ½çš„è¡¨æ ¼å†…å®¹
                if self._is_likely_table_content(text, table_bboxes):
                    continue
                
                # è·å–å…ƒç´ ç±»å‹
                element_type = type(element).__name__
                
                # å°è¯•è·å–é¡µç ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                page_number = getattr(element, 'metadata', {}).get('page_number', None)
                
                text_elements.append({
                    'content': text,
                    'element_type': element_type,
                    'page_number': page_number,
                    'char_count': len(text)
                })
            
            return text_elements
            
        except Exception as e:
            print(f"   âš ï¸  æ–‡æœ¬æå–å¤±è´¥: {e}")
            return []
    
    def _chunk_text_elements(self, text_elements: List[Dict]) -> List[TextChunk]:
        """Step 3: å¯¹æ–‡æœ¬å…ƒç´ è¿›è¡ŒäºŒæ¬¡åˆ†å—"""
        text_chunks = []
        
        for element in text_elements:
            content = element['content']
            
            # å¦‚æœæ–‡æœ¬è¾ƒçŸ­ï¼Œç›´æ¥ä½œä¸ºä¸€ä¸ªå­åˆ†å—
            if len(content) <= self.child_chunk_size:
                chunk = TextChunk(
                    chunk_id=f"text_{str(uuid.uuid4())[:8]}",
                    content=content,
                    page_number=element['page_number'],
                    element_type=element['element_type'],
                    parent_id="",  # åç»­åˆ†é…
                    char_count=len(content)
                )
                text_chunks.append(chunk)
            else:
                # å¯¹é•¿æ–‡æœ¬è¿›è¡Œé€’å½’åˆ†å—
                sub_chunks = self._recursive_text_split(content, element)
                text_chunks.extend(sub_chunks)
        
        return text_chunks
    
    def _recursive_text_split(self, text: str, element: Dict) -> List[TextChunk]:
        """é€’å½’åˆ†å‰²é•¿æ–‡æœ¬"""
        chunks = []
        
        # æ™ºèƒ½åˆ†å‰²ç‚¹
        separators = ['\n\n', '\n', 'ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?', ' ']
        
        start = 0
        while start < len(text):
            end = start + self.child_chunk_size
            
            if end >= len(text):
                # æœ€åä¸€å—
                chunk_text = text[start:].strip()
                if chunk_text:
                    chunk = TextChunk(
                        chunk_id=f"text_{str(uuid.uuid4())[:8]}",
                        content=chunk_text,
                        page_number=element['page_number'],
                        element_type=element['element_type'],
                        parent_id="",  # åç»­åˆ†é…
                        char_count=len(chunk_text)
                    )
                    chunks.append(chunk)
                break
            
            # å¯»æ‰¾æœ€ä½³åˆ†å‰²ç‚¹
            segment = text[start:end]
            best_split = -1
            
            for separator in separators:
                split_pos = segment.rfind(separator)
                if split_pos > self.child_chunk_size * 0.7:  # è‡³å°‘70%çš„ç›®æ ‡é•¿åº¦
                    best_split = start + split_pos + len(separator)
                    break
            
            if best_split == -1:
                best_split = end
            
            chunk_text = text[start:best_split].strip()
            if chunk_text:
                chunk = TextChunk(
                    chunk_id=f"text_{str(uuid.uuid4())[:8]}",
                    content=chunk_text,
                    page_number=element['page_number'],
                    element_type=element['element_type'],
                    parent_id="",  # åç»­åˆ†é…
                    char_count=len(chunk_text)
                )
                chunks.append(chunk)
            
            # ä¸‹ä¸€ä¸ªèµ·ç‚¹ï¼ˆè€ƒè™‘é‡å ï¼‰
            start = max(best_split - self.child_chunk_overlap, start + 1)
        
        return chunks
    
    def _build_parent_child_relationships(self, table_chunks: List[TableChunk], text_chunks: List[TextChunk]) -> List[ParentChunk]:
        """Step 4: æ„å»ºçˆ¶å­å…³ç³»"""
        parent_chunks = []
        
        if self.parent_strategy == 'page_group':
            parent_chunks = self._build_page_group_parents(table_chunks, text_chunks)
        elif self.parent_strategy == 'chapter':
            parent_chunks = self._build_chapter_parents(table_chunks, text_chunks)
        else:
            # é»˜è®¤ä½¿ç”¨é¡µé¢åˆ†ç»„
            parent_chunks = self._build_page_group_parents(table_chunks, text_chunks)
        
        return parent_chunks
    
    def _build_page_group_parents(self, table_chunks: List[TableChunk], text_chunks: List[TextChunk]) -> List[ParentChunk]:
        """æŒ‰é¡µé¢åˆ†ç»„æ„å»ºçˆ¶åˆ†å—"""
        parent_chunks = []
        
        # æ”¶é›†æ‰€æœ‰é¡µç 
        all_pages = set()
        for chunk in table_chunks:
            all_pages.add(chunk.page_number)
        for chunk in text_chunks:
            if chunk.page_number:
                all_pages.add(chunk.page_number)
        
        if not all_pages:
            return parent_chunks
        
        # æŒ‰é¡µé¢åˆ†ç»„
        sorted_pages = sorted(all_pages)
        
        for i in range(0, len(sorted_pages), self.pages_per_parent):
            page_group = sorted_pages[i:i + self.pages_per_parent]
            start_page = page_group[0]
            end_page = page_group[-1]
            
            parent_id = f"parent_pages_{start_page}_{end_page}_{str(uuid.uuid4())[:8]}"
            
            # æ”¶é›†è¿™ä¸ªé¡µé¢ç»„çš„æ‰€æœ‰å­åˆ†å—
            child_ids = []
            parent_content_parts = []
            
            # æ·»åŠ è¡¨æ ¼å­åˆ†å—
            for chunk in table_chunks:
                if chunk.page_number in page_group:
                    chunk.parent_id = parent_id
                    child_ids.append(chunk.chunk_id)
                    parent_content_parts.append(f"[è¡¨æ ¼ - é¡µé¢{chunk.page_number}]\n{chunk.content}\n")
            
            # æ·»åŠ æ–‡æœ¬å­åˆ†å—
            for chunk in text_chunks:
                if chunk.page_number in page_group:
                    chunk.parent_id = parent_id
                    child_ids.append(chunk.chunk_id)
                    parent_content_parts.append(chunk.content)
            
            # åˆ›å»ºçˆ¶åˆ†å—
            if child_ids:
                parent_chunk = ParentChunk(
                    parent_id=parent_id,
                    title=f"é¡µé¢ {start_page}-{end_page}",
                    content="\n\n".join(parent_content_parts),
                    page_range=(start_page, end_page),
                    child_ids=child_ids,
                    chunk_type="page_group"
                )
                parent_chunks.append(parent_chunk)
        
        return parent_chunks
    
    def _build_chapter_parents(self, table_chunks: List[TableChunk], text_chunks: List[TextChunk]) -> List[ParentChunk]:
        """æŒ‰ç« èŠ‚æ„å»ºçˆ¶åˆ†å—ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„ç« èŠ‚è¯†åˆ«é€»è¾‘
        # ç›®å‰å…ˆç”¨é¡µé¢åˆ†ç»„ä½œä¸ºæ›¿ä»£
        return self._build_page_group_parents(table_chunks, text_chunks)
    
    def _prepare_child_chunks_for_vector_db(self, table_chunks: List[TableChunk], text_chunks: List[TextChunk]) -> List[Dict]:
        """å‡†å¤‡å­åˆ†å—ç”¨äºå‘é‡æ•°æ®åº“å­˜å‚¨"""
        child_chunks = []
        
        # è¡¨æ ¼å­åˆ†å—
        for chunk in table_chunks:
            child_chunks.append({
                'chunk_id': chunk.chunk_id,
                'content': chunk.content,
                'chunk_type': 'table',
                'parent_id': chunk.parent_id,
                'metadata': {
                    'page_number': chunk.page_number,
                    'table_type': chunk.table_type,
                    'row_count': chunk.row_count,
                    'col_count': chunk.col_count,
                    'bbox': chunk.bbox
                }
            })
        
        # æ–‡æœ¬å­åˆ†å—
        for chunk in text_chunks:
            child_chunks.append({
                'chunk_id': chunk.chunk_id,
                'content': chunk.content,
                'chunk_type': 'text',
                'parent_id': chunk.parent_id,
                'metadata': {
                    'page_number': chunk.page_number,
                    'element_type': chunk.element_type,
                    'char_count': chunk.char_count
                }
            })
        
        return child_chunks
    
    def _prepare_parent_chunks_for_doc_store(self, parent_chunks: List[ParentChunk]) -> Dict[str, Dict]:
        """å‡†å¤‡çˆ¶åˆ†å—ç”¨äºæ–‡æ¡£å­˜å‚¨ï¼ˆé”®å€¼å¯¹ï¼‰"""
        parent_store = {}
        
        for chunk in parent_chunks:
            parent_store[chunk.parent_id] = {
                'parent_id': chunk.parent_id,
                'title': chunk.title,
                'content': chunk.content,
                'page_range': chunk.page_range,
                'child_ids': chunk.child_ids,
                'chunk_type': chunk.chunk_type
            }
        
        return parent_store
    
    def _dataframe_to_markdown(self, df: pd.DataFrame) -> str:
        """DataFrameè½¬Markdown"""
        if df.empty:
            return ""
        
        try:
            return df.to_markdown(index=False, tablefmt='pipe')
        except:
            # æ‰‹åŠ¨æ„å»ºMarkdownè¡¨æ ¼
            lines = []
            headers = [str(col) for col in df.columns]
            header_line = "| " + " | ".join(headers) + " |"
            lines.append(header_line)
            
            separator_line = "|" + "|".join([" --- " for _ in headers]) + "|"
            lines.append(separator_line)
            
            for _, row in df.iterrows():
                row_values = [str(val) if pd.notna(val) else "" for val in row]
                row_line = "| " + " | ".join(row_values) + " |"
                lines.append(row_line)
            
            return "\n".join(lines)
    
    def _classify_table_type(self, df: pd.DataFrame) -> str:
        """è¡¨æ ¼ç±»å‹åˆ†ç±»"""
        financial_keywords = [
            'è¥æ”¶', 'æ”¶å…¥', 'åˆ©æ¶¦', 'èµ„äº§', 'è´Ÿå€º', 'ç°é‡‘æµ', 'æ¯›åˆ©ç‡',
            'revenue', 'income', 'profit', 'asset', 'liability'
        ]
        
        table_text = df.to_string().lower()
        for keyword in financial_keywords:
            if keyword.lower() in table_text:
                return 'financial'
        
        return 'other'
    
    def _is_likely_table_content(self, text: str, table_bboxes: List[Dict]) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ˜¯è¡¨æ ¼å†…å®¹ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        # æ•°å­—å¯†åº¦æ£€æŸ¥
        numbers = re.findall(r'\d+', text)
        words = text.split()
        if len(words) > 0 and len(numbers) > len(words) * 0.6:
            return True
        
        # è¡¨æ ¼å…³é”®è¯æ£€æŸ¥
        table_keywords = ['è¥ä¸šæ”¶å…¥', 'å‡€åˆ©æ¶¦', 'èµ„äº§', 'è´Ÿå€º', 'ç°é‡‘æµ', 'æ¯›åˆ©ç‡', 'ä¸‡å…ƒ', 'åƒå…ƒ']
        keyword_count = sum(1 for keyword in table_keywords if keyword in text)
        if keyword_count >= 2:
            return True
        
        return False
    
    def save_results(self, result: Dict[str, Any], output_dir: str = "parent_child_rag_results"):
        """ä¿å­˜çˆ¶å­åˆ†å—RAGç»“æœ"""
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        result_file = output_path / f"parent_child_rag_{timestamp}.json"
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜å­åˆ†å—ï¼ˆç”¨äºå‘é‡æ•°æ®åº“ï¼‰
        child_chunks_file = output_path / f"child_chunks_{timestamp}.json"
        with open(child_chunks_file, 'w', encoding='utf-8') as f:
            json.dump(result['child_chunks'], f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜çˆ¶åˆ†å—ï¼ˆç”¨äºæ–‡æ¡£å­˜å‚¨ï¼‰
        parent_chunks_file = output_path / f"parent_chunks_{timestamp}.json"
        with open(parent_chunks_file, 'w', encoding='utf-8') as f:
            json.dump(result['parent_chunks'], f, ensure_ascii=False, indent=2)
        
        # ç”ŸæˆæŠ¥å‘Š
        report_file = output_path / f"parent_child_report_{timestamp}.md"
        self._generate_report(result, report_file)
        
        print(f"ğŸ’¾ çˆ¶å­åˆ†å—RAGç»“æœå·²ä¿å­˜åˆ°:")
        print(f"   ğŸ“‹ æŠ¥å‘Š: {report_file}")
        print(f"   ğŸ‘¶ å­åˆ†å—: {child_chunks_file}")
        print(f"   ğŸ‘¨ çˆ¶åˆ†å—: {parent_chunks_file}")
        print(f"   ğŸ’¾ å®Œæ•´æ•°æ®: {result_file}")
        
        return {
            'report_file': report_file,
            'child_chunks_file': child_chunks_file,
            'parent_chunks_file': parent_chunks_file,
            'result_file': result_file
        }
    
    def _generate_report(self, result: Dict[str, Any], report_file: Path):
        """ç”Ÿæˆçˆ¶å­åˆ†å—RAGæŠ¥å‘Š"""
        stats = result['stats']
        
        report_content = f"""# çˆ¶å­åˆ†å—RAGå¤„ç†æŠ¥å‘Š

## åŸºæœ¬ä¿¡æ¯
- **æ–‡ä»¶**: {result['metadata']['source_file']}
- **å¤„ç†æ—¶é—´**: {result['metadata']['processed_at']}
- **å¤„ç†æ–¹æ³•**: {result['metadata']['processing_method']}

## åˆ†å—ç»Ÿè®¡
- **å­åˆ†å—æ€»æ•°**: {stats['total_child_chunks']}
  - è¡¨æ ¼å­åˆ†å—: {stats['table_child_chunks']}
  - æ–‡æœ¬å­åˆ†å—: {stats['text_child_chunks']}
- **çˆ¶åˆ†å—æ€»æ•°**: {stats['parent_chunks']}
- **è·³è¿‡çš„è¡¨æ ¼åŒºåŸŸ**: {stats['table_bboxes_excluded']}

## çˆ¶å­åˆ†å—RAGæ¶æ„

### ğŸ¯ å¤„ç†æµç¨‹
1. **é«˜ä¿çœŸè¡¨æ ¼æå–** - pdfplumberæå–{stats['table_child_chunks']}ä¸ªè¡¨æ ¼ï¼Œè½¬ä¸ºMarkdownå­åˆ†å—
2. **é«˜ä¿çœŸæ–‡æœ¬æå–** - unstructuredè·³è¿‡{stats['table_bboxes_excluded']}ä¸ªè¡¨æ ¼åŒºåŸŸï¼Œæå–æ–‡æœ¬å…ƒç´ 
3. **æ–‡æœ¬äºŒæ¬¡åˆ†å—** - é€’å½’åˆ†å—ç”Ÿæˆ{stats['text_child_chunks']}ä¸ªæ–‡æœ¬å­åˆ†å—
4. **æ„å»ºçˆ¶å­å…³ç³»** - åˆ›å»º{stats['parent_chunks']}ä¸ªçˆ¶åˆ†å—ï¼Œå…³è”æ‰€æœ‰å­åˆ†å—
5. **RAGå­˜å‚¨å‡†å¤‡** - å­åˆ†å—ç”¨äºå‘é‡åŒ–ï¼Œçˆ¶åˆ†å—ç”¨äºä¸Šä¸‹æ–‡æ£€ç´¢

### ğŸ“Š å­˜å‚¨æ¶æ„
- **å‘é‡æ•°æ®åº“**: å­˜å‚¨{stats['total_child_chunks']}ä¸ªå­åˆ†å—çš„å‘é‡è¡¨ç¤º
- **æ–‡æ¡£å­˜å‚¨**: å­˜å‚¨{stats['parent_chunks']}ä¸ªçˆ¶åˆ†å—çš„å®Œæ•´å†…å®¹ï¼ˆé”®å€¼å¯¹ï¼‰

### ğŸ” æ£€ç´¢æµç¨‹
1. ç”¨æˆ·æé—® â†’ å‘é‡æ•°æ®åº“æ£€ç´¢ç›¸å…³å­åˆ†å—
2. ä»å­åˆ†å—å…ƒæ•°æ®æå–çˆ¶åˆ†å—ID
3. ä»æ–‡æ¡£å­˜å‚¨è·å–çˆ¶åˆ†å—å®Œæ•´å†…å®¹
4. å°†çˆ¶åˆ†å—å†…å®¹æäº¤ç»™LLMç”Ÿæˆç­”æ¡ˆ

## é…ç½®å‚æ•°
- **å­åˆ†å—å¤§å°**: {result['metadata']['config'].get('child_chunk_size', 800)}
- **å­åˆ†å—é‡å **: {result['metadata']['config'].get('child_chunk_overlap', 100)}
- **çˆ¶åˆ†å—ç­–ç•¥**: {result['metadata']['config'].get('parent_strategy', 'page_group')}
- **æ¯ç»„é¡µæ•°**: {result['metadata']['config'].get('pages_per_parent', 3)}

## RAGç³»ç»Ÿé›†æˆæŒ‡å—

### 1. å‘é‡æ•°æ®åº“å­˜å‚¨
```python
# å­˜å‚¨å­åˆ†å—åˆ°å‘é‡æ•°æ®åº“
for child_chunk in result['child_chunks']:
    vector = embed_text(child_chunk['content'])
    vector_db.store(
        id=child_chunk['chunk_id'],
        vector=vector,
        metadata=child_chunk['metadata']
    )
```

### 2. æ–‡æ¡£å­˜å‚¨
```python
# å­˜å‚¨çˆ¶åˆ†å—åˆ°æ–‡æ¡£å­˜å‚¨
for parent_id, parent_data in result['parent_chunks'].items():
    doc_store[parent_id] = parent_data['content']
```

### 3. æ£€ç´¢æŸ¥è¯¢
```python
# æ£€ç´¢æµç¨‹
def query_rag(question):
    # 1. æ£€ç´¢ç›¸å…³å­åˆ†å—
    child_chunks = vector_db.search(question, top_k=5)
    
    # 2. è·å–çˆ¶åˆ†å—ID
    parent_ids = [chunk['metadata']['parent_id'] for chunk in child_chunks]
    
    # 3. è·å–çˆ¶åˆ†å—å®Œæ•´å†…å®¹
    parent_contents = [doc_store[pid] for pid in set(parent_ids)]
    
    # 4. ç”Ÿæˆç­”æ¡ˆ
    return llm.generate(question, parent_contents)
```

## ä¼˜åŠ¿ç‰¹ç‚¹
- âœ… **é«˜ä¿çœŸæå–**: è¡¨æ ¼å’Œæ–‡æœ¬éƒ½ä¿æŒåŸå§‹ç»“æ„
- âœ… **é¿å…é‡å¤**: unstructuredè·³è¿‡è¡¨æ ¼åŒºåŸŸï¼Œé¿å…é‡å¤å¤„ç†
- âœ… **ç»†ç²’åº¦æ£€ç´¢**: å­åˆ†å—æä¾›ç²¾ç¡®çš„å‘é‡æ£€ç´¢
- âœ… **ä¸°å¯Œä¸Šä¸‹æ–‡**: çˆ¶åˆ†å—æä¾›å®Œæ•´çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
- âœ… **çµæ´»é…ç½®**: æ”¯æŒå¤šç§çˆ¶åˆ†å—ç­–ç•¥
- âœ… **RAGä¼˜åŒ–**: ä¸“é—¨ä¸ºRAGç³»ç»Ÿè®¾è®¡çš„å­˜å‚¨æ¶æ„

"""
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)

def demo_parent_child_rag():
    """æ¼”ç¤ºçˆ¶å­åˆ†å—RAGå¤„ç†"""
    print("ğŸš€ çˆ¶å­åˆ†å—RAGå¤„ç†æ¼”ç¤º")
    print("=" * 80)
    
    pdf_path = "data/reports/600580_å§é¾™ç”µé©±2025-04-26_å§é¾™ç”µé©±2024å¹´å¹´åº¦æŠ¥å‘Š_2105.pdf"
    
    if not Path(pdf_path).exists():
        print(f"âŒ PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
        return
    
    try:
        # é…ç½®å‚æ•°
        config = {
            'child_chunk_size': 800,
            'child_chunk_overlap': 100,
            'parent_strategy': 'page_group',
            'pages_per_parent': 3
        }
        
        processor = ParentChildRAGProcessor(config)
        
        # å¤„ç†PDF
        result = processor.process_pdf(pdf_path)
        
        # æ˜¾ç¤ºç»“æœ
        print("\nğŸ“Š çˆ¶å­åˆ†å—RAGç»“æœ:")
        stats = result['stats']
        print(f"  å­åˆ†å—æ€»æ•°: {stats['total_child_chunks']}")
        print(f"    - è¡¨æ ¼å­åˆ†å—: {stats['table_child_chunks']}")
        print(f"    - æ–‡æœ¬å­åˆ†å—: {stats['text_child_chunks']}")
        print(f"  çˆ¶åˆ†å—æ€»æ•°: {stats['parent_chunks']}")
        print(f"  è·³è¿‡è¡¨æ ¼åŒºåŸŸ: {stats['table_bboxes_excluded']}")
        
        # æ˜¾ç¤ºç¤ºä¾‹
        print("\nğŸ“‹ å­åˆ†å—ç¤ºä¾‹:")
        for i, chunk in enumerate(result['child_chunks'][:5]):
            print(f"  {i+1}. [{chunk['chunk_type']}] {chunk['chunk_id'][:20]}... (çˆ¶åˆ†å—: {chunk['parent_id'][:15]}...)")
        
        print("\nğŸ“‹ çˆ¶åˆ†å—ç¤ºä¾‹:")
        for i, (parent_id, parent_data) in enumerate(list(result['parent_chunks'].items())[:3]):
            print(f"  {i+1}. {parent_data['title']} - {len(parent_data['child_ids'])}ä¸ªå­åˆ†å—")
        
        # ä¿å­˜ç»“æœ
        files = processor.save_results(result)
        
        print("\nâœ… çˆ¶å­åˆ†å—RAGå¤„ç†å®Œæˆï¼")
        print("\nğŸ¯ RAGç³»ç»Ÿé›†æˆ:")
        print("  1. å°†child_chunks.jsonä¸­çš„å­åˆ†å—å‘é‡åŒ–å­˜å…¥å‘é‡æ•°æ®åº“")
        print("  2. å°†parent_chunks.jsonä¸­çš„çˆ¶åˆ†å—å­˜å…¥æ–‡æ¡£å­˜å‚¨")
        print("  3. æ£€ç´¢æ—¶ï¼šå­åˆ†å—æ£€ç´¢ â†’ çˆ¶åˆ†å—ä¸Šä¸‹æ–‡ â†’ LLMç”Ÿæˆç­”æ¡ˆ")
        
        return result
        
    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == '__main__':
    demo_parent_child_rag()